{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Forecast Data Collection\n",
    "### Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent\n",
    "import requests, json\n",
    "import re\n",
    "import os\n",
    "from time import sleep\n",
    "from datetime import datetime, timedelta, date as dt_date\n",
    "from random import uniform\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location Data\n",
    "We start by obtaining the the 150 most popular cities on Accuweather. Their free API does not permit more than a few days of forecast information (and only 50 requests are permitted a day). An api key is required to run the following code the first time, after which the recorded data loads from a saved file, which I have included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('data/accuweather_cities.json'):\n",
    "    with open(\"data/accuweather_cities.json\",\"r\") as f:\n",
    "        response=json.loads(f.read())\n",
    "else:\n",
    "    with open(\"data/accuweather_api.txt\", \"r\") as f:\n",
    "        api_key = f.readline()\n",
    "        \n",
    "    response = requests.get(\"http://dataservice.accuweather.com/locations/v1/topcities/150\",\n",
    "                                params = {'apikey':api_key })\n",
    "\n",
    "    response = response.json()\n",
    "\n",
    "    with open(\"data/accuweather_cities.json\",\"w\") as f:\n",
    "        f.write(json.dumps(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response) #150 locations as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data Collection\n",
    "### Exploration\n",
    "Given the locations, we wish to gather the available weather data for Jul 2020 until then the end of August 2020. We will need to explore how to scrape this data, given that the future predictions provided by the API are too short. Take the following examples of URLs which are obtained by navigating through the browser and compare to the data available via the API.\n",
    "\n",
    "https://www.accuweather.com/en/bd/dhaka/28143/july-weather/28143\n",
    "\n",
    "https://www.accuweather.com/en/bd/dhaka/28143/august-weather/28143\n",
    "\n",
    "https://www.accuweather.com/en/gb/london/ec4a-2/july-weather/328328\n",
    "\n",
    "https://www.accuweather.com/en/gb/london/ec4a-2/august-weather/328328\n",
    "\n",
    "We note that the urls for London also work if we substitute ec4a%202 for ec4a-2 (the former being consistent with the API). There are some differences between these two formats -- Dhaka repeats the string 328328 at two points of the URL, whereas London does not have a repeated numerical string. Let's explore why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Version': 1, 'Key': '28143', 'Type': 'City', 'Rank': 10, 'LocalizedName': 'Dhaka', 'EnglishName': 'Dhaka', 'PrimaryPostalCode': '', 'Region': {'ID': 'ASI', 'LocalizedName': 'Asia', 'EnglishName': 'Asia'}, 'Country': {'ID': 'BD', 'LocalizedName': 'Bangladesh', 'EnglishName': 'Bangladesh'}, 'AdministrativeArea': {'ID': 'C', 'LocalizedName': 'Dhaka', 'EnglishName': 'Dhaka', 'Level': 1, 'LocalizedType': 'Division', 'EnglishType': 'Division', 'CountryID': 'BD'}, 'TimeZone': {'Code': 'BDT', 'Name': 'Asia/Dhaka', 'GmtOffset': 6.0, 'IsDaylightSaving': False, 'NextOffsetChange': None}, 'GeoPosition': {'Latitude': 23.71, 'Longitude': 90.407, 'Elevation': {'Metric': {'Value': 5.0, 'Unit': 'm', 'UnitType': 5}, 'Imperial': {'Value': 16.0, 'Unit': 'ft', 'UnitType': 0}}}, 'IsAlias': False, 'SupplementalAdminAreas': [], 'DataSets': ['AirQualityCurrentConditions', 'AirQualityForecasts']}\n"
     ]
    }
   ],
   "source": [
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28143\n",
      "\n",
      "BD\n"
     ]
    }
   ],
   "source": [
    "print(response[0]['Key'])\n",
    "print(response[0]['PrimaryPostalCode'])\n",
    "print(response[0]['Country']['ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the two letter country code and numerical key present in the Dhaka URL. There is no value for `PrimaryPostalCode`, but I've included this as it will be relevant in a second. We can do the same for London, finding the position in the CSV first and then printing the relevant entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "regex = re.compile('London')\n",
    "\n",
    "for i in range(150):\n",
    "    if regex.match(response[i]['EnglishName']):\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328328\n",
      "EC4A 2\n",
      "GB\n"
     ]
    }
   ],
   "source": [
    "print(response[8]['Key'])\n",
    "print(response[8]['PrimaryPostalCode'])\n",
    "print(response[8]['Country']['ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that that we make use of the city name, country ID, primary postal code, if it exists, and key. If the primary postal code is an empty string, we simply use the key again in place of the primary postal code. There are two names, LocalizedName and EnglishName. We can check if there is ever a mismatch (since it's not clear which is used in the url). According to the below there is not -- for all cities in the top 150 list, the localised and English names are the same, as alleged by accuweather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(150):\n",
    "    if response[i]['EnglishName'] != response[i]['LocalizedName']:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scraping\n",
    "We now know where to find the data we need, so we will start to scrape. We define some auxiliary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_in_string(string):\n",
    "    \"\"\"\n",
    "    Returns first occurence of a month name in a string. Filters to ensure\n",
    "    that it is a complete name and not part of another word.\n",
    "    \"\"\"\n",
    "    months = ['january', 'february', 'march', 'april', 'may', 'june', 'july', \n",
    "              'august', 'september', 'october','november', 'december']\n",
    "    month_reg = re.compile('\\b'+'|'.join(months)+'\\b')\n",
    "    url_month = month_reg.search(string).group()\n",
    "    return url_month\n",
    "    \n",
    "def accudate_to_datetime(accudate, month):\n",
    "    \"\"\"\n",
    "    Accuweather dates on month display will be of the form m/d or just d, depending on\n",
    "    if the date is in the current calendar month: e.g. during July we get 6/28, 6/29/, 6/30,\n",
    "    1, 2, ... , 30, 31, 8/1, 8/2... Converts to datetime object.\n",
    "    \"\"\"\n",
    "    if '/' in accudate:\n",
    "        formatted_date = datetime.strptime(accudate + ' 2020', '%m/%d %Y')\n",
    "    else:\n",
    "        formatted_date = datetime.strptime(accudate +' '+ month +' 2020', '%d %B %Y')\n",
    "        \n",
    "    return formatted_date.date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a list of tuples, containing the country ID, postal code, and key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tuples(data = response):\n",
    "    tuple_list = []\n",
    "    for i in data:\n",
    "        tup = (i['Country']['ID'], i['EnglishName'], i['PrimaryPostalCode'], i['Key'])\n",
    "        tuple_list.append(tup)\n",
    "    return tuple_list\n",
    "\n",
    "url_entries = make_tuples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then populate a list with the urls obtained from appropriately formatting the urls with these tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_url(tup, url_list):\n",
    "    #handle absent post codes and adjust to lower case\n",
    "    new_tup = (tup[0].lower(), tup[1].lower(), tup[2] or tup[3], tup[3]) \n",
    "    \n",
    "    url_list.append('https://www.accuweather.com/en/{}/{}/{}/july-weather/{}'.format(*new_tup))\n",
    "    url_list.append('https://www.accuweather.com/en/{}/{}/{}/august-weather/{}'.format(*new_tup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = []\n",
    "for i in url_entries:\n",
    "    append_url(i, url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The timezones of the cities in the list fall within [GMT - 11, GMT + 12]. If we record between 11.00 and 12.00 every day, then this will ensure that we do not skip any days due to change of date because of timezone. Ideally I'd have a better way of scheduling the data collection task, but I only have a small laptop at my disposal. I've checked that in July and August we don't have to worry about DST ruining this strategy, so if I'm diligent, there should not be any missing data. For a safer margin, I'll sort the URLs by absolute deviation of timezone from GMT, so that those closest to chaning time zone should be collected first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz = [] # Timezone list\n",
    "for i in range(150):\n",
    "    tz.append((response[i]['TimeZone']['GmtOffset'],response[i]['EnglishName']))\n",
    "    tz.append((response[i]['TimeZone']['GmtOffset'],response[i]['EnglishName'])) # Append for each of 2 months\n",
    "    \n",
    "name_url_list = [(name,url) for (tz,name),url in sorted(zip(tz,url_list), reverse = True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we come to the task of collecting and parsing the data. The below function will return dates, hi/lo temperature and an icon code indicating precipitation or lack thereof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When testing, requests where denied unless I used a real user agent\n",
    "my_user = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19041'}\n",
    "\n",
    "def forecast_parser(url):\n",
    "    # Get raw data\n",
    "    month = month_in_string(url)\n",
    "    response = requests.get(url, headers = my_user).text\n",
    "    soup = BeautifulSoup(response)\n",
    "    # Divide into days (including weather info)\n",
    "    all_dates = soup.find_all(\"a\", class_=\"monthly-daypanel\")\n",
    "    \n",
    "    # If we just want temperatures: date_info = [date.get_text().split() for date in all_dates]   \n",
    "    # If we want preciptiation information, a bit more complicated, since this is only\n",
    "    # provided by image icons. See appendix. Below implements this correctly\n",
    "    \n",
    "    regex = re.compile('\\d+')\n",
    "    date_info = []\n",
    "    for date in all_dates:\n",
    "        text = date.get_text().split() # Contains temperature and date information\n",
    "        if date.img: # Only exists for present+future dates\n",
    "            img = regex.search(date.img['data-src']).group() # Labels precipitation icons\n",
    "            text.append(img)\n",
    "        date_info.append(text)\n",
    "    \n",
    "    # Clean up list. \n",
    "    # 1. Change date to datetime object, temperatures to ints\n",
    "    for date in date_info:\n",
    "        date[0] = accudate_to_datetime(date[0], month)\n",
    "        for i in range(1,len(date)):\n",
    "            date[i]=date[i].replace('°','')\n",
    "            try:\n",
    "                date[i]=int(date[i])\n",
    "            except:\n",
    "                pass\n",
    "    # 2. Only keep data which is not a string (deletes historical weather data and text)\n",
    "    # We do, however, append an empty string as a null value for absent precipitation data\n",
    "    date_info_clean = [[x for x in date if not isinstance(x,str)]+[''] for date in date_info]\n",
    "    \n",
    "    return date_info_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the main data collection task using the above function, and the following two steps\n",
    "\n",
    "1. Create a CSV file with appropriate labels of all the dates in the July and August. The columns represent collection date, location, data type (hi temp, lo temp or precipitation info) and then the prediction date.\n",
    "\n",
    "2. Parse the URLs. Concatenate the information for July and August then write 3 new rows to the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates file if it does not yet exist and formats its headers.\n",
    "\n",
    "if not os.path.exists('data/weather_data.csv'):\n",
    "    # Create CSV. Column labels. Probably some more efficient way to do with pandas.\n",
    "    date1 = '2020-07-21'\n",
    "    date2 = '2020-08-31'\n",
    "    start = datetime.strptime(date1, '%Y-%m-%d')\n",
    "    end = datetime.strptime(date2, '%Y-%m-%d')\n",
    "    step = timedelta(days=1)\n",
    "    date_string_list = []\n",
    "    while start <= end:\n",
    "        date_string_list.append(start.date())\n",
    "        start += step\n",
    "    date_string = ','.join(map(str,date_string_list))\n",
    "    first_row = 'Collected,'+'Location,'+'Type,'+date_string+'\\n'\n",
    "\n",
    "    with open('data/weather_data.csv', 'w+') as f:\n",
    "        f.write(first_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three strings for each location:\n",
    "def string_tuple(location, date_info_jul_raw, date_info_aug_raw):\n",
    "    now = datetime.now()\n",
    "    now_str = str(now)\n",
    "    \n",
    "    \n",
    "    # First date to be recorded is jul21. Filter dates to ensure no double counting.\n",
    "    jul21 = dt_date(2020,7,21)\n",
    "    date_info_jul = [x for x in date_info_jul_raw if x[0].month == 7 and x[0]>= jul21]\n",
    "    date_info_aug = [x for x in date_info_aug_raw if x[0].month == 8]\n",
    "\n",
    "    date_info_comb = date_info_jul + date_info_aug\n",
    "            \n",
    "    hi_temps = ','.join([now_str, location, 'high']+[str(x[1]) for x in date_info_comb])\n",
    "    lo_temps = ','.join([now_str, location, 'low']+[str(x[2]) for x in date_info_comb])\n",
    "    precip = ','.join([now_str, location, 'precipitation']+[str(x[3]) for x in date_info_comb])\n",
    "    return(hi_temps, lo_temps, precip)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Put it all together\n",
    "missed_locations = []\n",
    "with open('data/weather_data.csv', 'a') as f:\n",
    "    for i in range(150):\n",
    "            location = name_url_list[2*i][0]\n",
    "            url_jul = name_url_list[2*i][1]\n",
    "            url_aug = name_url_list[2*i + 1][1]\n",
    "\n",
    "            sleep(uniform(3,10)) # Reduces the chance of rate-limiting.\n",
    "\n",
    "            date_info_jul_raw = forecast_parser(url_jul)\n",
    "            sleep(uniform(2,5))\n",
    "            date_info_aug_raw = forecast_parser(url_aug)\n",
    "            try:\n",
    "                rows = string_tuple(location, date_info_jul_raw, date_info_aug_raw)\n",
    "                for i in rows:\n",
    "                    f.write(i+'\\n')\n",
    "            except:\n",
    "                missed_locations.append(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wellington', 'Auckland', 'Pago Pago']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check missing locations\n",
    "missed_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some places in extreme time zones are missing. We collect manually later in the day.\n",
    "date_info_jul_raw = forecast_parser(url_jul)\n",
    "sleep(uniform(2,5))\n",
    "date_info_aug_raw = forecast_parser(url_aug)\n",
    "rows = string_tuple(location, date_info_jul_raw, date_info_aug_raw)\n",
    "with open('data/weather_data.csv', 'a') as f:\n",
    "    for i in rows:\n",
    "        f.write(i+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of crash, resume from point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_locations = []\n",
    "def data_from_point(location):\n",
    "    for i in range(len(name_url_list)):\n",
    "        if name_url_list[i][0] == location:\n",
    "            start = i//2\n",
    "            break\n",
    "    with open('data/weather_data.csv', 'a') as f:\n",
    "        for i in range(start, 150):\n",
    "                location = name_url_list[2*i][0]\n",
    "                url_jul = name_url_list[2*i][1]\n",
    "                url_aug = name_url_list[2*i + 1][1]\n",
    "\n",
    "                sleep(uniform(3,10))\n",
    "\n",
    "                date_info_jul_raw = forecast_parser(url_jul)\n",
    "                sleep(uniform(2,5))\n",
    "                date_info_aug_raw = forecast_parser(url_aug)\n",
    "                try:\n",
    "                    rows = string_tuple(location, date_info_jul_raw, date_info_aug_raw)\n",
    "                    for i in rows:\n",
    "                        f.write(i+'\\n')\n",
    "                except:\n",
    "                    missed_locations.append(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_from_point(location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weaknesses\n",
    "\n",
    "- Accuweather's top 150 cities is biased towards larger cities (but does include some smaller ones)\n",
    "- The data only spans about 40 days, and hence doesn't span across multiple seasons for any given location.\n",
    "- The data was collected manually using a laptop, so although I tried to be as consistent as possible, it was not always possible to collect at the same time every day, and occasionally some data was missed.\n",
    "- There would be better ways to automate collecting missed data, and resuming after crashes. This should be investigated if I want to improve data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preciptation data can be deduced by the icon number codes. Accuweather icons, their corresponding numbers and text summary interpretations are available here:\n",
    "https://developer.accuweather.com/weather-icons. Below I collect and write to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_user = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19041'}\n",
    "response = requests.get('https://developer.accuweather.com/weather-icons', headers = my_user).text\n",
    "soup = BeautifulSoup(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preciptation_parser(soup):\n",
    "    table_rows = soup.find_all('tr')[1:]\n",
    "\n",
    "    icon_weather_dict = {}\n",
    "    for row in table_rows:\n",
    "        columns = row.find_all('td')\n",
    "        icon_num = int(columns[0].text.strip())\n",
    "        weather_text = columns[-1].text.strip()\n",
    "        icon_weather_dict[icon_num] = weather_text\n",
    "\n",
    "    return icon_weather_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_json = json.dumps(preciptation_parser(soup))\n",
    "# Writing to sample.json\n",
    "if not os.path.exists('data/precip.json'):\n",
    "    with open(\"data/precip.json\", \"w\") as f:\n",
    "        f.write(precip_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
